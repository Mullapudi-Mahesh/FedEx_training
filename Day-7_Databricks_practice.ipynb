{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d1fbdc-5f7f-4ad4-98e9-400bf80a4095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+------------+---------+\n| id|     city|temperature|  event_date|is_active|\n+---+---------+-----------+------------+---------+\n|  1|  Chennai|       34.7|  2024-01-01|     true|\n|  2|    Delhi|       33.6|  2024-01-01|     true|\n|  3|Bangalore|       NULL|INVALID_DATE|    false|\n|  4|Bangalore|       38.9|INVALID_DATE|     true|\n|  5|    Delhi|       27.5|INVALID_DATE|     NULL|\n+---+---------+-----------+------------+---------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"workspace.default.weather_raw\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d97d22-ec7f-4a2d-91bc-dffef4e6d242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = spark.table(\"workspace.default.weather_raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc25370b-aa5a-4f75-b30b-729a9558b2f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----------+---------+\n| id|     city|temperature|event_date|is_active|\n+---+---------+-----------+----------+---------+\n|  1|  Chennai|       34.7|2024-01-01|     true|\n|  2|    Delhi|       33.6|2024-01-01|     true|\n|  6|Bangalore|       31.9|2024-01-02|    false|\n|  7|Hyderabad|       25.0|2024-01-01|    false|\n|  8|Bangalore|       31.1|2024-01-02|    false|\n+---+---------+-----------+----------+---------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, try_to_date\n",
    "\n",
    "silver_df = (\n",
    "    bronze_df\n",
    "    .withColumn(\n",
    "        \"temperature\",\n",
    "        when(col(\"temperature\").isNull(), 25.0).otherwise(col(\"temperature\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"event_date\",\n",
    "        try_to_date(col(\"event_date\"), \"yyyy-MM-dd\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"is_active\",\n",
    "        when(col(\"is_active\").isNull(), False).otherwise(col(\"is_active\"))\n",
    "    )\n",
    "    .filter(col(\"event_date\").isNotNull())\n",
    ")\n",
    "\n",
    "silver_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c624aa-dc65-4e20-8d16-37aac19e3c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.default.weather_silver\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36f1b74-2bcd-4562-b719-5d2cab2c6f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All unit tests passed\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"workspace.default.weather_silver\")\n",
    "\n",
    "assert df.count() > 0, \" No data in silver table\"\n",
    "assert df.filter(col(\"temperature\").isNull()).count() == 0, \" Null temperatures\"\n",
    "assert df.filter(col(\"event_date\").isNull()).count() == 0, \" Invalid dates\"\n",
    "\n",
    "print(\" All unit tests passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bef74ec-0c85-4841-8baf-1d759943df4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 10:57:47,491 | INFO | weather_pipeline | Pipeline execution started\n2026-01-27 10:57:47,492 | INFO | weather_pipeline | Reading Bronze table: workspace.default.weather_raw\n2026-01-27 10:57:47,974 | INFO | weather_pipeline | Bronze row count: 100\n2026-01-27 10:57:47,975 | INFO | weather_pipeline | Starting Silver transformations\n2026-01-27 10:57:48,732 | INFO | weather_pipeline | Silver row count after cleaning: 67\n2026-01-27 10:57:49,328 | INFO | weather_pipeline | Null temperature count (should be 0): 0\n2026-01-27 10:57:49,329 | INFO | weather_pipeline | Writing Silver table: workspace.default.weather_silver\n2026-01-27 10:57:53,632 | INFO | weather_pipeline | Silver table written successfully\n2026-01-27 10:57:53,632 | INFO | weather_pipeline | Pipeline execution completed in 6.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from pyspark.sql.functions import col, when, try_to_date\n",
    "\n",
    "logger = logging.getLogger(\"weather_pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "start_time = time.time()\n",
    "logger.info(\"Pipeline execution started\")\n",
    "\n",
    "try:\n",
    "    logger.info(\"Reading Bronze table: workspace.default.weather_raw\")\n",
    "    bronze_df = spark.table(\"workspace.default.weather_raw\")\n",
    "\n",
    "    bronze_count = bronze_df.count()\n",
    "    logger.info(f\"Bronze row count: {bronze_count}\")\n",
    "\n",
    "    if bronze_count == 0:\n",
    "        logger.warning(\"Bronze table is empty\")\n",
    "\n",
    "    logger.info(\"Starting Silver transformations\")\n",
    "\n",
    "    silver_df = (\n",
    "        bronze_df\n",
    "        .withColumn(\n",
    "            \"temperature\",\n",
    "            when(col(\"temperature\").isNull(), 25.0).otherwise(col(\"temperature\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"event_date\",\n",
    "            try_to_date(col(\"event_date\"), \"yyyy-MM-dd\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_active\",\n",
    "            when(col(\"is_active\").isNull(), False).otherwise(col(\"is_active\"))\n",
    "        )\n",
    "        .filter(col(\"event_date\").isNotNull())\n",
    "    )\n",
    "\n",
    "    silver_count = silver_df.count()\n",
    "    logger.info(f\"Silver row count after cleaning: {silver_count}\")\n",
    "\n",
    "    null_temp_count = silver_df.filter(col(\"temperature\").isNull()).count()\n",
    "    logger.info(f\"Null temperature count (should be 0): {null_temp_count}\")\n",
    "\n",
    "    if null_temp_count > 0:\n",
    "        logger.warning(\"Unexpected null temperatures detected\")\n",
    "\n",
    "    logger.info(\"Writing Silver table: workspace.default.weather_silver\")\n",
    "\n",
    "    silver_df.write.mode(\"overwrite\").saveAsTable(\n",
    "        \"workspace.default.weather_silver\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"Silver table written successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Pipeline execution failed\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    logger.info(f\"Pipeline execution completed in {duration} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}